{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/chridam/dev/work/datatalk/mlops-zoomcamp/.venv/lib/python3.9/site-packages (3.5.2)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/chridam/dev/work/datatalk/mlops-zoomcamp/.venv/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T16:10:11.236334Z",
     "start_time": "2024-09-10T16:10:09.251969Z"
    }
   },
   "id": "76f6475d873953a3",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour, dayofweek, month, year"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T16:10:11.285963Z",
     "start_time": "2024-09-10T16:10:11.238739Z"
    }
   },
   "id": "initial_id",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Food Delivery Data Exploration\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def load_parquet_files(spark, path):\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def basic_statistics(df, column):\n",
    "    return df.select(column).summary().toPandas()\n",
    "\n",
    "def plot_distribution(df, column, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=column, kde=True)\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"{column}_distribution.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_time_series(df, time_column, value_column, title):\n",
    "    df = df.sort_values(time_column)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[time_column], df[value_column])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(value_column)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f\"{value_column}_time_series.png\")\n",
    "    plt.close()\n",
    "\n",
    "def analyze_order_placed_events(spark, path):\n",
    "    df = load_parquet_files(spark, os.path.join(path, \"order_placed_events\"))\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(basic_statistics(df, \"totalAmount\"))\n",
    "    \n",
    "    # Distribution of order amounts\n",
    "    pandas_df = df.select(\"totalAmount\").toPandas()\n",
    "    plot_distribution(pandas_df, \"totalAmount\", \"Distribution of Order Amounts\")\n",
    "    \n",
    "    # Time series of order frequency\n",
    "    time_df = df.groupBy(hour(\"timestamp\").alias(\"hour\")).count().toPandas()\n",
    "    plot_time_series(time_df, \"hour\", \"count\", \"Order Frequency by Hour\")\n",
    "    \n",
    "    # Most popular restaurants\n",
    "    top_restaurants = df.groupBy(\"restaurantId\").count().orderBy(col(\"count\").desc()).limit(10).toPandas()\n",
    "    print(\"Top 10 Restaurants by Order Count:\")\n",
    "    print(top_restaurants)\n",
    "\n",
    "def analyze_order_delivery_events(spark, path):\n",
    "    df = load_parquet_files(spark, os.path.join(path, \"order_delivery_events\"))\n",
    "    \n",
    "    # Calculate delivery time\n",
    "    df = df.withColumn(\"delivery_time\", (col(\"actualDeliveryTime\").cast(\"long\") - col(\"timestamp\").cast(\"long\")) / 60)\n",
    "    \n",
    "    # Basic statistics of delivery time\n",
    "    print(basic_statistics(df, \"delivery_time\"))\n",
    "    \n",
    "    # Distribution of delivery times\n",
    "    pandas_df = df.select(\"delivery_time\").toPandas()\n",
    "    plot_distribution(pandas_df, \"delivery_time\", \"Distribution of Delivery Times (minutes)\")\n",
    "    \n",
    "    # Average delivery time by hour of day\n",
    "    time_df = df.groupBy(hour(\"timestamp\").alias(\"hour\")).avg(\"delivery_time\").orderBy(\"hour\").toPandas()\n",
    "    plot_time_series(time_df, \"hour\", \"avg(delivery_time)\", \"Average Delivery Time by Hour\")\n",
    "\n",
    "def analyze_review_events(spark, path):\n",
    "    df = load_parquet_files(spark, os.path.join(path, \"review_events\"))\n",
    "    \n",
    "    # Basic statistics of ratings\n",
    "    print(\"Food Rating Statistics:\")\n",
    "    print(basic_statistics(df, \"foodRating\"))\n",
    "    print(\"Delivery Rating Statistics:\")\n",
    "    print(basic_statistics(df, \"deliveryRating\"))\n",
    "    print(\"Overall Rating Statistics:\")\n",
    "    print(basic_statistics(df, \"overallRating\"))\n",
    "    \n",
    "    # Distribution of ratings\n",
    "    pandas_df = df.select(\"foodRating\", \"deliveryRating\", \"overallRating\").toPandas()\n",
    "    plot_distribution(pandas_df, \"foodRating\", \"Distribution of Food Ratings\")\n",
    "    plot_distribution(pandas_df, \"deliveryRating\", \"Distribution of Delivery Ratings\")\n",
    "    plot_distribution(pandas_df, \"overallRating\", \"Distribution of Overall Ratings\")\n",
    "    \n",
    "    # Correlation between ratings\n",
    "    correlation_matrix = pandas_df.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(\"Correlation between Ratings\")\n",
    "    plt.show()\n",
    "    plt.savefig(\"rating_correlation.png\")\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T16:33:29.586276Z",
     "start_time": "2024-09-10T16:33:29.574945Z"
    }
   },
   "id": "bc2be8d30254fd7d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for Parquet at . It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m spark \u001B[38;5;241m=\u001B[39m create_spark_session()\n\u001B[1;32m      2\u001B[0m parquet_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput/events\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Update this path\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43manalyze_order_placed_events\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparquet_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# analyze_order_delivery_events(spark, parquet_path)\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# analyze_review_events(spark, parquet_path)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m spark\u001B[38;5;241m.\u001B[39mstop()\n",
      "Cell \u001B[0;32mIn[12], line 33\u001B[0m, in \u001B[0;36manalyze_order_placed_events\u001B[0;34m(spark, path)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21manalyze_order_placed_events\u001B[39m(spark, path):\n\u001B[0;32m---> 33\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mload_parquet_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morder_placed_events\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;66;03m# Basic statistics\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mprint\u001B[39m(basic_statistics(df, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "Cell \u001B[0;32mIn[12], line 7\u001B[0m, in \u001B[0;36mload_parquet_files\u001B[0;34m(spark, path)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_parquet_files\u001B[39m(spark, path):\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/work/datatalk/mlops-zoomcamp/.venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:544\u001B[0m, in \u001B[0;36mDataFrameReader.parquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    533\u001B[0m int96RebaseMode \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint96RebaseMode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m    535\u001B[0m     mergeSchema\u001B[38;5;241m=\u001B[39mmergeSchema,\n\u001B[1;32m    536\u001B[0m     pathGlobFilter\u001B[38;5;241m=\u001B[39mpathGlobFilter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    541\u001B[0m     int96RebaseMode\u001B[38;5;241m=\u001B[39mint96RebaseMode,\n\u001B[1;32m    542\u001B[0m )\n\u001B[0;32m--> 544\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/dev/work/datatalk/mlops-zoomcamp/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/dev/work/datatalk/mlops-zoomcamp/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Unable to infer schema for Parquet at . It must be specified manually."
     ]
    }
   ],
   "source": [
    "    spark = create_spark_session()\n",
    "    parquet_path = \"output/events\"  # Update this path\n",
    "    \n",
    "    analyze_order_placed_events(spark, parquet_path)\n",
    "    # analyze_order_delivery_events(spark, parquet_path)\n",
    "    # analyze_review_events(spark, parquet_path)\n",
    "    \n",
    "    spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T16:36:03.762702Z",
     "start_time": "2024-09-10T16:36:03.629271Z"
    }
   },
   "id": "6425acd1e2dc0cea",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "193d0ba4733970c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
